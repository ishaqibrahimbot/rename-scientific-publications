<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Multimodal Opinion Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-27">27 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinbae</forename><surname>Im</surname></persName>
							<email>jinbae@ncsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Knowledge AI Lab</orgName>
								<orgName type="institution">NCSOFT Co</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moonki</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Knowledge AI Lab</orgName>
								<orgName type="institution">NCSOFT Co</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hoyeop</forename><surname>Lee</surname></persName>
							<email>hoyeoplee@ncsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Knowledge AI Lab</orgName>
								<orgName type="institution">NCSOFT Co</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyunsouk</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Knowledge AI Lab</orgName>
								<orgName type="institution">NCSOFT Co</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sehee</forename><surname>Chung</surname></persName>
							<email>seheechung@ncsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Knowledge AI Lab</orgName>
								<orgName type="institution">NCSOFT Co</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Multimodal Opinion Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-27">27 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.13135v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2021-05-30T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary. However, non-text data such as image and metadata related to reviews have been considered less often. To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called Mul-timodalSum. Our framework obtains a representation of each modality using a separate encoder for each modality, and the text decoder generates a summary. To resolve the inherent heterogeneity of multimodal data, we propose a multimodal training pipeline. We first pretrain the text encoder-decoder based solely on text modality data. Subsequently, we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data. Finally, to fuse multimodal representations, we train the entire framework in an end-to-end manner. We demonstrate the superiority of MultimodalSum by conducting experiments on Yelp and Amazon datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Opinion summarization is the task of automatically generating summaries from multiple documents containing users' thoughts on businesses or products. This summarization of users' opinions can provide information that helps other users with their decision-making on consumption. Unlike conventional single-document or multiple-document summarization, where we can obtain the prevalent annotated summaries <ref type="bibr" target="#b34">(Nallapati et al., 2016;</ref><ref type="bibr" target="#b40">See et al., 2017;</ref><ref type="bibr" target="#b37">Paulus et al., 2018;</ref><ref type="bibr" target="#b38">Perez-Beltrachini et al., 2019)</ref>, opinion summarization is challenging; it is difficult to find summarized opinions of users. Accordingly,  studies used an unsupervised approach for opinion summarization <ref type="bibr" target="#b23">(Ku et al., 2006;</ref><ref type="bibr" target="#b36">Paul et al., 2010;</ref><ref type="bibr" target="#b5">Carenini et al., 2013;</ref><ref type="bibr" target="#b14">Ganesan et al., 2010;</ref><ref type="bibr" target="#b15">Gerani et al., 2014)</ref>. Recent studies <ref type="bibr" target="#b0">Amplayo and Lapata, 2020;</ref><ref type="bibr" target="#b11">Elsahar et al., 2021</ref>) used a self-supervised learning framework that creates a synthetic pair of source reviews and a pseudo summary by sampling a review text from a training corpus and considering it as a pseudo summary, as in Figure <ref type="figure" target="#fig_1">1a</ref>.</p><p>Users' opinions are based on their perception of a specific entity and perceptions originate from various characteristics of the entity; therefore, opinion summarization can use such characteristics. For instance, Yelp provides users food or menu images and various metadata about restaurants, as in <ref type="bibr">Figure 1b</ref>. This non-text information influences the review text generation process of users <ref type="bibr" target="#b42">(Truong and Lauw, 2019)</ref>. Therefore, using this additional information can help in opinion summarization, especially under unsupervised settings <ref type="bibr" target="#b41">(Su et al., 2019;</ref><ref type="bibr" target="#b19">Huang et al., 2020)</ref>. Furthermore, the training process of generating a review text (a pseudo summary) based on the images and metadata for self-supervised learning is consistent with the ac-tual process of writing a review text by a user.</p><p>This study proposes a self-supervised multimodal opinion summarization framework called MultimodalSum by extending the existing selfsupervised opinion summarization framework, as shown in Figure <ref type="figure" target="#fig_1">1</ref>. Our framework receives source reviews, images, and a table on the specific business or product as input and generates a pseudo summary as output. Note that images and the table are not aligned with an individual review in the framework, but they correspond to the specific entity. We adopt the encoder-decoder framework and build multiple encoders representing each input modality. However, a fundamental challenge lies in the heterogeneous data of various modalities <ref type="bibr" target="#b2">(Baltrušaitis et al., 2018)</ref>.</p><p>To address this challenge, we propose a multimodal training pipeline. The pipeline regards the text modality as a pivot modality. Therefore, we pretrain the text modality encoder and decoder for a specific business or product via the self-supervised opinion summarization framework. Subsequently, we pretrain modality encoders for images and a table to generate review texts belonging to the same business or product using the pretrained text decoder. When pretraining the non-text modality encoders, the pretrained text decoder is frozen so that the image and table modality encoders obtain homogeneous representations with the pretrained text encoder. Finally, after pretraining input modalities, we train the entire model in an end-to-end manner to combine multimodal information.</p><p>Our contributions can be summarized as follows:</p><p>• this study is the first work on self-supervised multimodal opinion summarization; • we propose a multimodal training pipeline to resolve the heterogeneity between input modalities; • we verify the effectiveness of our model framework and model training pipeline through various experiments on Yelp and Amazon datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Generally, opinion summarization has been conducted in an unsupervised manner, which can be divided into extractive and abstractive approaches. The extractive approach selects the most meaningful texts from input opinion documents, and the abstractive approach generates summarized texts that are not shown in the input documents. Most previ-ous works on unsupervised opinion summarization have focused on extractive approaches. Clusteringbased approaches <ref type="bibr" target="#b6">(Carenini et al., 2006;</ref><ref type="bibr" target="#b23">Ku et al., 2006;</ref><ref type="bibr" target="#b36">Paul et al., 2010;</ref><ref type="bibr" target="#b1">Angelidis and Lapata, 2018)</ref> were used to cluster opinions regarding the same aspect and extract the text representing each cluster. Graph-based approaches <ref type="bibr" target="#b12">(Erkan and Radev, 2004;</ref><ref type="bibr" target="#b33">Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b47">Zheng and Lapata, 2019)</ref> were used to construct a graph-where nodes were sentences, and edges were similarities between sentences-and extract the sentences based on their centrality.</p><p>Although some abstractive approaches were not based on neural networks <ref type="bibr" target="#b14">(Ganesan et al., 2010;</ref><ref type="bibr" target="#b15">Gerani et al., 2014;</ref><ref type="bibr" target="#b10">Di Fabbrizio et al., 2014)</ref>, neural network-based approaches have been gaining attention recently. <ref type="bibr" target="#b8">Chu and Liu (2019)</ref> generated an abstractive summary from a denoising autoencoder-based model. More recent abstractive approaches have focused on self-supervised learning.  randomly selected N review texts for each entity and constructed N synthetic pairs by sequentially regarding one review text as a pseudo summary and the others as source reviews. Amplayo and Lapata (2020) sampled a review text as a pseudo summary and generated various noisy versions of it as source reviews. <ref type="bibr" target="#b11">Elsahar et al. (2021)</ref> selected review texts similar to the sampled pseudo summary as source reviews, based on TF-IDF cosine similarity. We construct synthetic pairs based on  and extend the self-supervised opinion summarization to a multimodal version.</p><p>Multimodal text summarization has been mainly studied in a supervised manner. Text summaries were created by using other modality data as additional input <ref type="bibr" target="#b26">(Li et al., , 2020a</ref>, and some studies provided not only a text summary but also other modality information as output <ref type="bibr" target="#b7">Chen and Zhuge, 2018;</ref><ref type="bibr" target="#b50">Zhu et al., 2020;</ref><ref type="bibr" target="#b28">Li et al., 2020b;</ref><ref type="bibr" target="#b13">Fu et al., 2020)</ref>. Furthermore, most studies summarized a single sentence or document. Although <ref type="bibr" target="#b26">Li et al. (2020a)</ref> summarized multiple documents, they used non-subjective documents. Our study is the first unsupervised multimodal text summarization work that summarizes multiple subjective documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>The goal of the self-supervised multimodal opinion summarization is to generate a pseudo sum-mary from multimodal data. Following existing self-supervised opinion summarization studies, we consider a review text selected from an entire review corpus as a pseudo summary. We extend the formulation of  to a multimodal version. Let R = {r 1 , r 2 , ..., r N } denote the set of reviews about an entity (e.g., a business or product). Each review, r j , consists of review text, d j , and review rating, s j , that represents the overall sentiment of the review text. We denote images uploaded by a user or provided by a company for the entity as I = {i 1 , i 2 , ..., i M } and a table containing abundant metadata about the entity as T . Here, T consists of several fields, and each field contains its own name and value. We set j-th review text d j as the pseudo summary and let it be generated from R −j , I, and T , where R −j = {r 1 , ..., r j−1 , r j+1 , ..., r N } denotes source reviews. To help the model summarize what stands out overall in the review corpus, we calculate the loss for all N cases of selecting d j from R, and train the model using the average loss. During testing, we generate a summary from R, I, and T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Framework</head><p>The proposed model framework, MultimodalSum, is designed with an encoder-decoder structure, as in Figure <ref type="figure" target="#fig_1">1b</ref>. To address the heterogeneity of three input modalities, we configure each modality encoder to effectively process data in each modality. We set a text decoder to generate summary text by synthesizing encoded representations from the three modality encoders. Details are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Encoder and Decoder</head><p>Our text encoder and decoder are based on BART <ref type="bibr" target="#b25">(Lewis et al., 2020)</ref>. BART is a Transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> encoder-decoder pretrained model that is particularly effective when fine-tuned for text generation and has high summarization performance. Furthermore, because the pseudo summary of self-supervised multimodal opinion summarization is an individual review text (d j ), we determine that pretraining BART based on a denoising autoencoder is suitable for our framework. Therefore, we further pretrain BART using the entire training review corpus <ref type="bibr" target="#b16">(Gururangan et al., 2020)</ref>. Our text encoder obtains e D -dimensional encoded text representations h text from D −j and the text decoder generates d j from h text as follows:</p><formula xml:id="formula_0">h text = BART enc (D −j ), d j = BART dec (h text ),</formula><p>where D −j = {d 1 , ..., d j−1 , d j+1 , ..., d N } denotes the set of review texts from R −j . Each review text consists of l D tokens and h text ∈ R (N −1)×l D ×e D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Encoder</head><p>We use a convolutional neural network specialized in analyzing visual imagery. In particular, we use ImageNet pretrained ResNet101 , which is widely used as a backbone network. We add an additional linear layer in place of the image classification layer to match feature distribution and dimensionality with text modality representations. Our image encoder obtains encoded image representations h img from I as follows:</p><formula xml:id="formula_1">h img = ResNet101(I) W img ,</formula><p>where W img ∈ R e I ×e D denotes the additional linear weights. h img obtains R M ×l I ×e D , where l I represents the size of the flattened image feature map obtained from ResNet101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Table Encoder</head><p>To effectively encode metadata, we design our table encoder based on the framework of data-to-text research <ref type="bibr" target="#b39">(Puduppully et al., 2019</ref> </p><formula xml:id="formula_2">f k = ReLU([n k ; v k ] W f + b f ), h table = F W table ,</formula><p>where n and v denote e T -dimensional representations of field name and value, respectively, and W f ∈ R 2e T ×e T , b f ∈ R e T are parameters. By stacking l T field representations, we obtain F ∈ R 1×l T ×e T . The additional linear weights W table ∈ R e T ×e D play the same role as in the image encoder, and h table ∈ R 1×l T ×e D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Training Pipeline</head><p>To effectively train the model framework, we set a model training pipeline, which consists of three steps, as in Figure <ref type="figure" target="#fig_2">2</ref>. The first step is text modality pretraining, in which a model learns unsupervised summarization capabilities using only text modality data. Next, during the pretraining for other modalities, an encoder for each modality is trained using the text modality decoder learned in the previous step as a pivot. The main purpose of this step is that other modalities have representations whose distribution is similar to that of the text modality. In the last step, the entire model framework is trained using all the modality data. Details of each step can be found in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text Modality Pretraining</head><p>In this step, we pretrain the text encoder and decoder for self-supervised opinion summarization.</p><p>As this was an important step for unsupervised multimodal neural machine translation <ref type="bibr" target="#b41">(Su et al., 2019)</ref>, we apply it to our framework. For the set of reviews about an entity R, we train the model to generate a pseudo summary d j from source reviews R −j for all N cases as follows: loss = N j=1 log p(d j |R −j ). The text encoder obtains h text ∈ R (N −1)×l D ×e D from D −j , and the text decoder aggregates the encoded representations of N − 1 review texts to generate d j . We model the aggregation of multiple encoded representations in the multi-head self-attention layer of the text decoder. To generate a pseudo summary that covers the overall contents of source reviews, we simply average the N − 1 single-head attention results for each encoded representation (R l D ×e D ) at each head <ref type="bibr" target="#b11">(Elsahar et al., 2021)</ref>.</p><p>The limitation of the self-supervised opinion summarization is that training and inference tasks are different. The model learns a review generation task using a review text as a pseudo summary; however, the model needs to perform a summary generation task at inference. To close this gap, we use a rating deviation between the source reviews and the target as an additional input feature of the text decoder, inspired by . We define the average ratings of the source reviews minus the rating of the target as the rating deviation: sd j = N i =j s i /(N − 1) − s j . We use sd j to help generate a pseudo summary d j during training and set it as 0 to generate a summary with average semantic of input reviews during inference. To reflect the rating deviation, we modify the way in which a Transformer creates input embeddings, as in Figure <ref type="figure" target="#fig_3">3</ref>. We create deviation embeddings with the same dimensionality as token embeddings and add sd j × deviation embeddings to the token embeddings in the same way as positional embeddings.</p><p>Our methods to close the gap between training and inference tasks do not require additional modeling or training in comparison with previous works. We achieve noising and denoising effects by simply using rating deviation embeddings without variational inference in . Furthermore, the information that the rating deviation is 0 plays the role of an input prompt for inference, without the need to train a separate classifier for selecting control tokens to be used as input prompts <ref type="bibr" target="#b11">(Elsahar et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Other Modalities Pretraining</head><p>As the main modality for summarization is the text modality, we pretrain the image and table encoders by pivoting the text modality. Although the data of the three modalities are heterogeneous, each encoder should be trained to obtain homogeneous representations. We achieve this by using the pretrained text decoder as a pivot. We train the image encoder and the table encoder along with the text decoder to generate a review text of the entity to which images or metadata belong: I or T → d j ∈ R. The image and table encoders obtain h img and h table from I and T , respectively, and the text decoder generates d j from h img or h table . Note that we aggregate M encoded representations of h img as in the text modality pretraining, and the weights of the text decoder are made constant. I or T corresponds to all N reviews, and this means that I or T has multiple references. We convert a multiplereference setting to a single-reference setting to match the model output with the text modality pretraining. We simply create N single reference pairs from each entity and shuffle pairs from all entities to construct the training dataset <ref type="bibr" target="#b48">(Zheng et al., 2018)</ref>. As the text decoder was trained for generating a review text from text encoded representations, the image and table encoders are bound to produce similar representations with the text encoder to generate the same review text. In this way, we can maximize the ability to extract the information necessary for generating the review text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training for Multiple Modalities</head><p>We train the entire multimodal framework from the pretrained encoders and decoder. The encoder of each modality obtains an encoded representation for each modality, and the text decoder generates the pseudo summary d j from multimodal encoded representations h text , h img , and h table . To fuse multimodal representations, we aim to meet three requirements. First, the text modality, which is the main modality, is primarily used. Second, the model works even if images or metadata are not available. Third, the model makes the most of the legacy from pretraining. To fulfill the requirements, multi-modality fusion is applied to the multi-head self-attention layer of the text decoder. The text decoder obtains the attention result for each modality at each layer. We fuse the attention results for multiple modalities as follows:  where ma text , ma img , and ma table denote each modality attention result from h text , h img , and h table , respectively. symbolizes elementwise multiplication and e D -dimensional multimodal gates α and β are calculated as follows:</p><formula xml:id="formula_3">ma fused = ma text + α ma img + β ma</formula><formula xml:id="formula_4">α = φ([ma text ; ma img ] W α ) and β = φ([ma text ; ma table ] W β ).</formula><p>Note that α or β obtains the zero vector when images or metadata do not exist. It is common to use sigmoid as an activation function φ. However, it can lead to confusion in the text decoder pretrained using only the text source. Because the values of W are initialized at approximately 0, the values of α and β are initialized at approximately 0.5 when sigmoid is used. To initialize the gate values at approximately 0, we use ReLU(tanh(x)) as φ(x). This enables the continuous use of text information, and images or metadata are used selectively.</p><p>6 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>To evaluate the effectiveness of the model framework and training pipeline on datasets with different domains and characteristics, we performed experiments on two review datasets: Yelp Dataset Challenge 1 and Amazon product reviews <ref type="bibr" target="#b18">(He and McAuley, 2016)</ref>. The Yelp dataset provides reviews based on personal experiences for a specific business. It also provides numerous images (e.g., food and drinks) uploaded by the users. Note that the maximum number of images, M , was set to 10 based on the 90 th percentile. In addition, the dataset contains abundant metadata of businesses according to the characteristics of each business. On the contrary, the Amazon dataset provides reviews with more objective and specific details about a particular product. It contains a sin-gle image provided by the supplier, and provides relatively limited metadata for the product. For evaluation, we used the data used in previous research <ref type="bibr" target="#b8">(Chu and Liu, 2019;</ref>. The data were generated by Amazon Mechanical Turk workers who summarized 8 input review texts. Therefore, we set N to 9 so that a pseudo summary is generated from 8 source reviews during training. For the Amazon dataset, 3 summaries are given per product. Simple data statistics are shown in Table <ref type="table" target="#tab_2">1</ref>, and other details can be found in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Details</head><p>All the models 2 were implemented with Py-Torch <ref type="bibr" target="#b35">(Paszke et al., 2019)</ref>, and we used the Transformers library from Hugging Face <ref type="bibr" target="#b45">(Wolf et al., 2020)</ref> as the backbone skeleton. Our text encoder and decoder were initialized using BART-Large and further pretrained using the training review corpus with the same objective as BART. e D , e I , and e T were all set to 1,024. We trained the entire models using the Adam optimizer (Kingma and Ba, 2014) with a linear learning rate decay on NVIDIA V100s. We decayed the model weights with 0.1. For each training pipeline, we set different batch sizes, epochs, learning rates, and warmup steps according to the amount of learning required at each step. We used label smoothing with 0.1 and set the maximum norm of gradients as 1 for other modalities pretraining and multiple-modalities training. During testing, we used beam search with early stopping and discarded hypotheses that contain twice the same trigram. Different beam size, length penalty, and max length were set for Yelp and Amazon. The best hyperparameter values and other details are described in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison Models</head><p>We compared our model to extractive and abstractive opinion summarization models. For extractive models, we used some simple baseline models . Clustroid selects one review that gets the highest ROUGE-L score with the other reviews of an entity. Lead constructs a summary by extracting and concatenating the lead sentences from all review texts of an entity. Random simply selects one random review from an entity. LexRank <ref type="bibr" target="#b12">(Erkan and Radev, 2004</ref>) is an extractive model that selects the most salient sentences based on graph centrality.</p><p>For abstractive models, we used non-neural and neural models. Opinosis <ref type="bibr" target="#b14">(Ganesan et al., 2010</ref>) is a non-neural model that uses a graph-based summarizer based on token-level redundancy. Mean-Sum <ref type="bibr" target="#b8">(Chu and Liu, 2019</ref>) is a neural model that is based on a denoising-autoencoder and generates a summary from mean representations of source reviews. We also used three self-supervised abstractive models. DenoiseSum (Amplayo and Lapata, 2020) generates a summary by denoising source reviews. Copycat (Bražinskas and Titov, 2020) uses a hierarchical variational autoencoder model and generates a summary from mean latent codes of the source reviews. Self &amp; Control <ref type="bibr" target="#b11">(Elsahar et al., 2021)</ref> generates a summary from Transformer models and uses some control tokens as additional inputs to the text decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>We evaluated our model framework and model training pipeline. In particular, we evaluated the summarization quality compared to other baseline models in terms of automatic and human evaluation, and conducted ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Automatic Evaluation</head><p>To evaluate the summarization quality, we used two automatic measures: ROUGE-{1,2,L} <ref type="bibr" target="#b29">(Lin, 2004)</ref> and BERT-score . The former is a token-level measure for comparing 1, 2, and adaptive L-gram matching tokens, and the latter is a document-level measure using pretrained BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. Contrary to ROUGEscore, which is based on exact matching between n-gram words, BERT-score is based on the semantic similarity between word embeddings that reflect the context of the document through BERT. It is approved that BERT-score is more robust to adversarial examples and correlates better with human judgments compared to other measures for machine translation and image captioning. We hypothesize that BERT-score is strong in opinion summarization as well, and BERT-score would complement ROUGE-score.</p><p>The results for opinion summarization on two datasets are shown in Table <ref type="table">2</ref>. MultimodalSum showed superior results compared with extractive and abstractive baselines for both token-level and document-level measures. From the results, we</p><formula xml:id="formula_5">Yelp Amazon Model R-1 R-2 R-L FBERT R-1 R-2 R-L FBERT Extractive</formula><p>Clustroid   Table <ref type="table">2</ref>: Opinion summarization results on Yelp and Amazon datasets. R-1, R-2, R-L, and F BERT refer to ROUGE-{1,2,L}, and BERT-score, respectively. The best models are marked in bold, and the second-best models are underlined. * indicates that our model shows significant gains (p &lt; 0.05) over the second-best model based on paired bootstrap resampling <ref type="bibr" target="#b22">(Koehn, 2004)</ref>. All the reported scores are based on F1.</p><p>Gold Wow, where to start? Some of the best sweet foods I've ever had. I wasn't sure what to try, so I tried a few things, and oh my goodness they were delicious. That's not all though, they serve drinks too so I got a latte and that was good too. There is a lot of variety here to choose from that'll make any sweet tooth salivate. Definitely a good place!</p><p>Copycat If you're looking for a sweet tooth this is the place to go if you want a delicious dessert. I had the lemon meringue pie and it was delicious. The only thing I didn't like was that I could eat half of it, but it was a little pricey for what you get.</p><p>Self &amp; Control If you're a fan of the Matos Buffet, this is a good place to visit. If you want to have a sweet tooth this is the place to be. The desserts are delicious and they have a good variety of desserts to choose from. The only thing I don't like about this place is that you have to wait in line for a long time to get in. Other than that, you can't really go wrong with any of the desserts in the buffet. The drinks are good and the desserts are yummy too. They also have desserts that are not too sweet. I'm not a huge fan of buffets, but this is one of my favorite buffets. MultimodalSum This is a cute little bakery located in the M resort. I had the chocolate croissant and it was very good. The croissants were soft and moist and the filling was delicious. I also had a chocolate chip cookie which was also good. I would definitely recommend this place if you are in the area. conclude that the multimodal framework outperformed the unimodal framework for unsupervised opinion summarization. In particular, our model achieved state-of-the-art results on the Amazon dataset and outperformed the comparable model by a large margin in the R-L representing the ROUGE scores on the Yelp dataset. Although Self &amp; Control showed high R-2 score, we attributed their score to the inferred N -gram control tokens used as additional inputs to the text decoder.</p><p>Sample summaries on the Yelp dataset are shown in Table <ref type="table" target="#tab_4">3</ref>. They were generated from source reviews on Baby Cakes bakery. Copycat misused "sweet tooth" and generated "lemon mernigue pie" that was not mentioned in the source reviews. Self &amp; Control generated a summary about a buffet by totally misunderstanding one sentence from source reviews: "If you love the desserts in Studio B Buffet in the M Hotel but don't want to wait in the massive buffet line or even eat in the buffet, Baby Cakes in the M Hotel is really nice fix." Furthermore, "Matos Buffet" is a non-existent word. On the contrary, MultimodalSum generated a good summary with a rich description of chocolate croissants. Although "chocolate chip cookie" was not found in the source reviews, our model generated it from cookie images. Note that the term can be found in other reviews that were not used as source reviews. Additional sample summaries on two datasets are shown in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Human Evaluation</head><p>To evaluate the quality of summarization based on human criteria, we conducted a user study. We assessed the quality of summaries using Best-Worst Scaling (BWS; Louviere et al. ( <ref type="formula">2015</ref>)). BWS is known to produce more reliable results than raking scales <ref type="bibr" target="#b21">(Kiritchenko and Mohammad, 2017)</ref> and is widely used in self-supervised opinion summarization studies. We recruited 10 NLP experts and asked each participant to choose one best and one worst summary from four summaries for three criteria. For each participant's response, the best model received +1, the worst model received -1, and the rest of the models received 0 scores. The final scores were obtained by averaging the scores of all the responses from all participants. From the table and two images, our model generates a summary. Heatmaps represent the overall influence of table and images for generating each word in the summary. Note that the summary is a real example generated from our model without beam search.</p><p>For Overall criterion, Self &amp; Control, Copycat, MultimodalSum, and gold summaries scored -0.527, -0.113, +0.260, and +0.380 on the Yelp dataset, respectively. MultimodalSum showed superior performance in human evaluation as well as automatic evaluation. We note that human judgments correlate better with BERT-score than ROUGE-score. Self &amp; Control achieved a very low human evaluation score despite its high ROUGEscore in automatic evaluation. We analyzed the summaries of Self &amp; Control, and we found several flaws such as redundant words, ungrammatical expressions, and factual hallucinations. It generated a non-existent word by combining several subwords. It was particularly noticeable when a proper noun was generated. Furthermore, Self &amp; Control generated an implausible sentence by copying some words from source reviews. From the results, we conclude that both automatic evaluation and human evaluation performances should be supported to be a good summarization model and BERT-score can complement ROUGE-score in automatic evaluation. Details on human evaluation and full results can be found in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Effects of Multimodality</head><p>To analyze the effects of multimodal data on opinion summarization, we analyzed the multimodal gate. Since the multimodal gate is a e Ddimensional vector, we averaged it by a scalar value. Furthermore, as multimodal gates exist for each layer of the text decoder, we averaged them to measure the overall influence of a table or images when generating each token in the decoder. An example of aggregated multimodal gates is shown in Figure <ref type="figure" target="#fig_4">4</ref>. It shows the table and images used for generating a summary text, and the multimodal gates for a part of the generated summary are expressed as heatmaps. As we intended, table and image information was selectively used to generate a specific word in the summary. The aggregated value of the table was relatively high for generating "Red Lobster", which is the name of the restaurants. It was relatively high for images, when generating "food" that is depicted in two images. Another characteristic of the result is that aggregated values of the table were higher than those of the image: mean values for the table and image in the entire test data were 0.103 and 0.045, respectively. This implies that table information is more used when creating a summary, and this observation is valid in that the table contains a large amount of metadata.</p><p>Note that the values displayed on the heatmaps are small by and large, as they were aggregated from e D -dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ablation Studies</head><p>For ablation studies, we analyzed the effectiveness of our model framework and model training pipeline in Table <ref type="table" target="#tab_6">4</ref>. To analyze the model framework, we first compared the summarization quality with four versions of unimodal model framework, as in the first block of Table <ref type="table" target="#tab_6">4</ref>. BART denotes the model framework in Figure <ref type="figure" target="#fig_1">1a</ref>, whose weights are the weights of BART-Large. It represents the lower bound of our model framework without any training. BART-Review denotes the model framework whose weights are from further pretrained BART using the entire training review corpus. Unimodal-Sum refers to the results of the text modality pretraining, and we classified it into two frameworks according to the use of the rating deviation. Surprisingly, using only BART achieved comparable or better results than many extractive and abstractive baselines in Table <ref type="table">2</ref>. Furthermore, further pretraining using the review corpus brought performance improvements. Qualitatively, BART with further pretraining generated more diverse words and rich expressions from the review corpus. This proved our assumption that denoising autoencoderbased pretraining helps in self-supervised multimodal opinion summarization. Based on the BART-Review, UnimodalSum achieved superior results. Furthermore, the use of rating deviation improved the quality of summarization. We conclude that learning to generate reviews based on wide ranges of rating deviations including 0 during training helps to generate a better summary of the average semantics of the input reviews.</p><p>To analyze the effect of other modalities in our model framework, we compared the summarization quality with three versions of multimodal model frameworks, as in the second block of Table 4. We removed the image or table modality from MultimodalSum to analyze the contribution of each modality. Results showed that both modalities improved the summarization quality compared with UnimodalSum, and they brought additional improvements when used altogether. This indicates that using non-text information helps in selfsupervised opinion summarization. As expected, the utility of the table modality was higher than that of the image modality. The image modality contains detailed information not revealed in the table modality (e.g., appearance of food, inside/outside mood of business, design of product, and color/texture of product). However, the information is unorganized to the extent that the utility of the image modality depends on the capacity of the image encoder to extract unorganized information. Although MultimodalSum used a representative image encoder because our study is the first work on multimodal opinion summarization, we expect that the utility of the image modality will be greater if unorganized information can be extracted effectively from the image using advanced image encoders.</p><p>For analyzing the model training pipeline, we removed text modality or/and other modalities pretraining from the pipeline. By removing each of them, the performance of MultimodalSum declined, and removing all of the pretraining steps caused an additional performance drop. Although Multi-  modalSum without other modalities pretraining has the capability of text summarization, it showed low summarization performance at the beginning of the training due to the heterogeneity of the three modality representations. However, MultimodalSum without text modality pretraining, whose image and table encoders were pretrained using BART-Review as a pivot, showed stable performance from the beginning, but the performance did not improve significantly. From the results, we conclude that both text modality and other modalities pretraining help the training of multimodal framework. For the other modalities pretraining, we conducted a further analysis in the Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We proposed the first self-supervised multimodal opinion summarization framework. Our framework can reflect text, images, and metadata together as an extension of the existing self-supervised opinion summarization framework. To resolve the heterogeneity of multimodal data, we also proposed a multimodal training pipeline. We verified the effectiveness of our multimodal framework and training pipeline with various experiments on real review datasets. Self-supervised multimodal opinion summarization can be used in various ways in the future, such as providing a multimodal summary or enabling a multimodal retrieval. By retrieving reviews related to a specific image or metadata, controlled opinion summarization will be possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Preprocessing</head><p>We selected businesses and products with a minimum of 10 reviews and popular entities above the 90 th percentile were removed. The minimum and maximum length of the words were set as 35 and 100 for Yelp, and 45 and 70 for Amazon, respectively. We set the maximum number of tokens as 128 using the BART tokenizer for training, and we did not limit the maximum tokens for inference.</p><p>For the Amazon dataset, we selected 4 categories: Electronics; Clothing, Shoes and Jewelry; Home and Kitchen; Health and Personal Care. As Yelp dataset contains unlimited number of images for each entity, we did not use images for popular entities above the 90 th percentile. On the other hand, Amazon dataset contains a single image for each entity. Therefore, we did not use images only when meaningless images such as non-image icon or update icon were used or the image links had expired.</p><p>For Yelp dataset, we selected name, ratings, categories, hours, and attributes among the metadata. We used the hours of each day of the week as seven fields and used all metadata contained in attributes as each field. For some attributes ('Ambience', 'BusinessParking', 'GoodForMeal') that have subordinate attributes, we used each subordinate attribute. Among the fields, we selected 47 fields used by at least 10% of the entities. We set the maximum number of categories as 6 based on the 90 th percentile, and averaged the representations of each category. For ratings, we converted it to binary notation consisting of 4 digits (2 2 , 2 1 , 2 0 , 2 −1 ). For hours, we considered (open hour, close hour) as a 2-dimensional vector, and conducted K-means clustering. We selected four clusters based on silhouette score: (16.5, 23.2), (8.7, 17.1), (6.4, 23), and (10.6, 22.6). Based on the clusters, we converted hours into a categorical type.</p><p>For Amazon dataset, we selected six fields: name, price, brand, categories, ratings, and description. We set the maximum number of categories as 3 based on the 90 th percentile, and averaged the representations of each category. Furthermore, as each category consists of hierarchies with a maximum of 8 depths, we averaged the representations of hierarchies to get each category representation. For price and ratings, we converted them to binary notation consisting of 11 and 4 digits, respectively, after rounding them to the nearest 0.5 to contain digit for 2 −1 . As some descriptions consist of many  tokens, we set the maximum number of tokens as 128. We regarded each token in description as each field, so we got total 5 + 128 fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Experimental Details</head><p>Our image encoder is based on ResNet101.</p><p>ResNet101 is composed of 1 convolution layer, 4 convolution layer blocks, and 1 fully connected layer block. Among them, 4 convolution layer blocks play an important role in analyzing image.</p><p>Through each convolution layer block, the size of the image feature map is reduced to 1/4, but it gets high-level features. To maintain the ability to extract low-level features of the image, we set the model weights up to the second convolution layer block not to be trained further. We only used up to the third convolution layer block to increase the resolution of feature maps without using too highlevel features for image classification. In this way, l I was set to 14 × 14 and e I was set to 1,024.</p><p>To use the knowledge of text modality in table encoder, we obtained field name embeddings by summing the BART token embeddings for the tokens contained in the field name. Because various data types can be used for field value, we used different processing methods for each data type. Nominal values were handled in the same way as the field name. Binary and ordinal values were processed by replacing them with nominal values of corresponding meanings: 'true' and 'false' were used for binary values, and 'cheap', 'average', 'expensive', and 'very expensive' were used for 'RestaurantsPriceRange'. Numerical values were converted to binary notation, and we obtained the representations by summing embeddings corresponding to the place, where the place value is 1. For other categorical values, we simply trained embeddings corresponding to each category.</p><p>We set each hyperparameter value different for each step in the model training pipeline, as in Table <ref type="table" target="#tab_8">5</ref>. We set the batch size according to the memory usage and set other values according to the amount of learning required. Hyperparameter ranges for epochs and lr (learning rate) were <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">10,</ref><ref type="bibr">15,</ref><ref type="bibr">20]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Human Evaluation</head><p>For human evaluation, we randomly selected 30 entities from Yelp test data, and used three criteria: Grmmaticality (the summary should be fluent and grammatical), Coherence (the summary should be well structured and well organized), and Overall (based on your own criteria, select the best and the worst summary of the reviews). Results for three criteria are shown in Table <ref type="table" target="#tab_10">6</ref>. Self &amp; Control achieved very poor performance for all criteria due to its flaws that were not revealed in the automatic evaluation. Surprisingly, MultimodalSum outperformed gold summaries for two criteria; however, its overall performance lagged behind Gold. As our model was initialized from BART-Large that had been pretrained using large corpus and further pretrained using training review corpus, it may have generated fluent and coherent summaries. It seems that our model lagged behind Gold in Overall due to various criteria other than those two. The fact that Gold scored lower than Copycat in Grammaticality may seem inconsistent with the result from . However, we assumed that this result was due to a combination of the four models in relative evaluation. The ranking for Copycat and Gold may have changed in absolute evaluation.</p><p>Image  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Analysis on Other Modalities Pretraining</head><p>To analyze the various models for the other modalities pretraining, we evaluated the performance of the reference review generation task that generates corresponding reviews from images or a table. For evaluation, we used the data that were not used for training data: we left 10% of the data for Yelp and 5% for Amazon. We chose two com-  <ref type="table" target="#tab_12">7</ref>. For each model, the pretrained decoder generated a review from image or table encoded representations. We measured the average ROUGE scores between the generated review and N reference reviews. The first finding was that results of table outperformed those of image. It indicates that table has more helpful information for generating reference review. The second finding was that our method based on the text decoder outperformed the Triplet based on the text encoder. Especially, Triplet achieved very poor performance for image because it is hard to match M images to N reference reviews for metric learning. On the contrary, our method achieved much better performance by pivoting the text decoder. Triplet showed good performance on table because it is relatively easy to match 1 table to N reference reviews; however, our method outperformed it. We conclude that our method lets the image and table encoder get proper representations to generate reference reviews regardless of the number of inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Example Summaries</head><p>Table <ref type="table" target="#tab_14">8</ref>, 9 show sample summaries generated from our model and baseline models on Yelp and Amazon datasets. Full summaries from our model are available at https://bit.ly/3bR4yod.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review 1</head><p>The fresh water catfish is probably the best I've every had. The service was outstanding. I would recommend this little secret to everyone.</p><p>Review 2</p><p>I loved everything about this place!! Great food, great decor, and great service. The best collard greens I have ever had. We had fried oysters for a starter and although I have never had them before so I have nothing to compare them with they were very tasty. The warm hush puppies with the honey butter was delicious!! I had the crab legs which were perfect and plentiful. My sister had the all you can eat fried catfish that was also cooked perfectly. A great experience all around!! Review 3</p><p>Amazing food and great service! The hospitality was out of this world. Will definitely be back soon.</p><p>The wait was less than 5 minutes at 7pm on a Friday night, amazing!! The staff was very kind and the waitresses were very attentive and helpful. We tried the frog legs, catfish, alligator bites, crab legs, gumbo and of course the hush puppies! Everything was outstanding. What a hidden gem! Review 4 I love this place the food amazing the staff helpful ....must try green tomatos ...fresh water fish ;ˆ)</p><p>Review 5</p><p>We love this place the catfish is good the hush puppies with that honey butter are awesome the french fries the gumbo what else is good there the alligator tail mostly everything on the menu. I guess the only bad thing I can say is sometimes it's like a 20 minute wait in the drive-through but it's well worth it when your food is hot Because tonight I got to go home and warm it up it's not hot enough, Even though they're still open for another hour that was a bummer Review 6</p><p>Really tasty catfish, shrimp and fixin's. Our friend took us to the sister location on Nellis a couple of months ago, but this location was more convenient to our hotel. No worries, this place was just as good! Excellent service, and the salad bar is a nice touch as well. As a Bostonian, I'm pretty particular about seafood. The Hush Puppy fits the bill. Very satisfied!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review 7</head><p>First Time here and the food, staff was awesome. Manager came over and gave us samples of the fried catfish, super nice.</p><p>Review 8 I never eat catfish. It's nasty to me until I tasted the saltwater catfish!!! Greens are on point. The hushpuppy are bomb with honey butter!!!!! Gator bites where are ok.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Copycat</head><p>This place is awesome! The food was great, the service was great. We had the catfish po'boy and it was delicious. The only reason I didn't give 5 stars is because of the fact that they don't deliver.</p><p>Self &amp; Control I love this place. The service is awesome. The hush puppies are to die for. I love the honey butter. I can't wait to go back and try it again. The only thing I don't like about the place is the wait. It can be a little long, but it's worth it. It's a little on the pricey side, but you're getting what you pay for. Love the hot butter, the hush puppies, the French fries, the gumbo, the catfish and the gumbo. Everything is so yummy and the service is top notch. Try it out, you won't be disappointed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultimodalSum</head><p>This place is a hidden gem. The food is great and the service is even better. I had the all you can eat catfish and it was delicious. The hush puppies are the best I've ever had. I will definitely be back.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold</head><p>Yummy and delicious catfish. You gotta try it. Friendly staff and service is good too. You can tell they know their seafood and how to prepare and cook it to perfection. The staff also answered any questions I had. The Hush Puppies are tasty too. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review 2</head><p>This is a really cute shoe that feels very comfortable on my high arches. The strap on the instep fits my feet very well, but I have very slim feet. I can see how it would be uncomfortably tight on anyone with more padding on their feet.</p><p>Review 3</p><p>I love these sandals. The fit is perfect for my foot, with perfect arch support. I don't think the leather is cheap, and the sandals are very comfortable to walk in. They are very pretty, and pair very well with pants and dresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review 4</head><p>My wife is a nurse and wears dansko shoes. We were excited to try the new crimson sandal and normally order 39 sandal and 40 closed toe. Some other reviews were right about a narrow width and tight toe box. We gave them a try and passed a great pair of shoes to our daughter with her long narrow feet, and she loves them...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Review 5</head><p>Finally, a Dansko sandal that's fashion forward! It was love at first sight! This is my 4th Dansko purchase.</p><p>Their sizing, quality and comfort is very consistent. I love the stying of this sandal and I'm pleased they are offering bolder colors. Another feature I love is the Dri-Lex topsole -it's soft and keeps feet dry.</p><p>Review 6 I really love these sandals. my only issue is after wearing them for a while my feet started to swell as I have a high instep and they were a little tight across the top. I'm sure they will stretch a bit after a few wears Review 7</p><p>I have several pairs of Dansko clogs that are all size 39 and fit perfectly. So I felt confident when I ordered the Tasha Sandal in size 39. I don't know if a 40 would be too large but the 39 seems a little small. Otherwise, I love them. They are very cushiony and comfortable! Review 8 I own many Dansko shoes and these are among my favorites. They have ALL the support that Dansko offers in its shoes plus they are very attractive. I love the the heel height and instant comfort. They look great with slacks and dresses, dressed up or not...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Copycat</head><p>This is my second pair of Dansko clogs and I love them. They are very comfortable and I can wear them all day without any discomfort. I would recommend them to anyone looking for a comfortable sandal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MultimodalSum</head><p>I love these sandals. They are very comfortable and look great. The only thing I don't like is that they are a little tight across the top of my foot. I have a high instep and the strap is a little too tight. I am hoping they will stretch out a bit.</p><p>Gold 1 I love these sandals, Dansko has made a really great product! I had to return my first pair (39) for being a bit tight and small, but I went a size higher (40) and it is perfect, they are so comfortable! If they do stretch out like other reviews say, they will still fit and look great.</p><p>Gold 2 I love these Dansko Tasha sandals! They are comfortable and the style is really cute. The only warning I have is that they seem to run narrow: you may want to buy a larger size if you have wide feet. Also, they seem to stretch as you wear them, so don't get discouraged by a few blisters on first wearing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold 3</head><p>These Dansko shoes are amazingly comfortable and hug the shape of my feet well, but I did have to wear them for a bit to stretch them out. They felt a little tight at first, but now they are perfect. I feel they're true to size so I'd recommend ordering these in your normal shoe size.</p><p>Table <ref type="table">9</ref>: Amazon summaries generated by different models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Self-supervised opinion summarization frameworks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Self-supervised multimodal opinion summarization training pipeline. Blurred boxes in "Other modalities pretraining" indicate that the text decoders are untrained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Text decoder input representations. The input embeddings are the sum of the token embeddings, rating deviation times deviation embeddings, and the positional embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Multimodal gate heatmaps; From the table and two images, our model generates a summary. Heatmaps represent the overall influence of table and images for generating each word in the summary. Note that the summary is a real example generated from our model without beam search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>value. The encoded table representations h table is obtained by stacking each field representa- tion into F and adding a linear layer as follows:</head><label></label><figDesc></figDesc><table><row><cell>). The input to</cell></row><row><cell>our table encoder T is a series of field-name and</cell></row><row><cell>field-value pairs. Each field gets e T -dimensional</cell></row><row><cell>representations through a multilayer perceptron af-</cell></row><row><cell>ter concatenating the representations of field-name</cell></row><row><cell>and field-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>table ,</head><label>,</label><figDesc></figDesc><table><row><cell>Yelp</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>#businesses</cell><cell>50,113</cell><cell>100</cell><cell>100</cell></row><row><cell>#reviews/business</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell>#summaries/business</cell><cell>1*</cell><cell>1</cell><cell>1</cell></row><row><cell>#max images</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>#max fields</cell><cell>47</cell><cell>47</cell><cell>47</cell></row><row><cell>Amazon</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>#products</cell><cell>60,935</cell><cell>28</cell><cell>32</cell></row><row><cell>#reviews/product</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell>#summaries/product</cell><cell>1*</cell><cell>3</cell><cell>3</cell></row><row><cell>#max images</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>#max fields</cell><cell>5+128</cell><cell cols="2">5+128 5+128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Data statistics; 1* in Train column indicates that it is a pseudo summary.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Sample summaries generated by various models on the Yelp dataset</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies on the Yelp dataset.</figDesc><table><row><cell>The first</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter values for each step in model training pipeline.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation results in terms of the BWS on the Yelp dataset.</figDesc><table><row><cell>respectively, and optimized values were chosen</cell></row><row><cell>from validation loss in one trial. For summary</cell></row><row><cell>generation at test time, we set different hyperpa-</cell></row><row><cell>rameter values for each dataset. Beam size, length</cell></row><row><cell>penalty, and max length were set to 4, 0.97, and</cell></row><row><cell>105 for Yelp and 2, 0.9, and 80 for Amazon, re-</cell></row><row><cell>spectively. Note that max length was set first to</cell></row><row><cell>prevent incomplete termination and length penalty</cell></row><row><cell>was determined based on the ROUGE scores on</cell></row><row><cell>validation dataset. The number of training parame-</cell></row><row><cell>ters for text, image, and table modality pretraining</cell></row><row><cell>are 406.3M, 27.1M, and 3.2M, respectively, and</cell></row><row><cell>that for multimodal training is 486.9M. Run time</cell></row><row><cell>for text modality pretraining was 16h on 4 GPUs,</cell></row><row><cell>and it took 41h and 43h on 2 GPUs for image and</cell></row><row><cell>table modality training, respectively. For final mul-</cell></row><row><cell>timodal training, it took 14h on 8 GPUs.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Reference reviews generation results on the Yelp dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Yelp summaries generated by different models. size 37, but found a 38 feels better in this sandal. I absolutely love this sandal. So supportive and comfortable, although at first I did get a blister on my big toe. Do not let this be the deciding factor. It stretched out and is now fabulous. I love it so much that I bought it in three colors.</figDesc><table><row><cell>I usually wear</cell></row><row><cell>Review 1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.yelp.com/dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our code is available at https://bit.ly/3bR4yod</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised opinion summarization with noising and denoising</title>
		<author>
			<persName><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised</title>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3675" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multimodal machine learning: A survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Few-shot learning for opinion summarization</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Bražinskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4119" to="4135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised opinion summarization as copycatreview generation</title>
		<author>
			<persName><forename type="first">Mirella</forename><forename type="middle">Lapata</forename><surname>Bražinskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5151" to="5169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-document summarization of evaluative tex</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="576" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-document summarization of evaluative text</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics</title>
				<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstractive textimage summarization using multi-modal attentional hierarchical rnn</title>
		<author>
			<persName><forename type="first">Jingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhuge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4046" to="4056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meansum: a neural model for unsupervised multi-document abstractive summarization</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1223" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hybrid approach to multidocument summarization of opinions in reviews</title>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Di Fabbrizio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Natural Language Generation Conference</title>
				<meeting>the 8th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised and controlled multi-document opinion summarization</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1646" to="1662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal summarization for video-containing documents</title>
		<author>
			<persName><forename type="first">Xiyan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08018</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opinosis: A graph based approach to abstractive summarization of highly redundant opinions</title>
		<author>
			<persName><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
				<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive summarization of product reviews using discourse structure</title>
		<author>
			<persName><forename type="first">Shima</forename><surname>Gerani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bita</forename><surname>Nejat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1602" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
				<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised multimodal neural machine translation with pseudo visual pivoting</title>
		<author>
			<persName><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8226" to="8237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bestworst scaling more reliable than rating scales: A case study on sentiment intensity annotation</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Opinion extraction, summarization and tracking in news and blog corpora</title>
		<author>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ting</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI spring symposium: Computational approaches to analyzing weblogs</title>
				<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="100" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Gang Hua, Houdong Hu, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pretraining for natural language generation</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aspect-aware multimodal summarization for chinese e-commerce products</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence</title>
				<meeting>the 34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8188" to="8195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-modal sentence summarization with modality attention and image filtering</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4152" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Vmsmo: Learning to generate multimodal summary for videobased news articles</title>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangming</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9360" to="9369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
				<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Best-worst scaling: Theory, methods and applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony Alfred John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><surname>Marley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2004 conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Summarizing contrastive viewpoints in opinionated text</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2010 conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating summaries with topic templates and structured convolutional decoders</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5107" to="5116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th AAAI Conference on Artificial Intelligence</title>
				<meeting>the 33th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6908" to="6915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised multi-modal neural machine translation</title>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10482" to="10491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multimodal review generation for recommender systems</title>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><surname>Lauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference</title>
				<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1864" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="494" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sentence centrality revisited for unsupervised summarization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6236" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-reference training with pseudo-references for neural translation and text generation</title>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3188" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Msmo: Multimodal summarization with multimodal output</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4154" to="4164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multimodal summarization with guidance of multimodal reference</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence</title>
				<meeting>the 34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9749" to="9756" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
